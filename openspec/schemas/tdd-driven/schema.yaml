name: tdd-driven
version: 1
description: TDD workflow â€” proposal â†’ specs â†’ design â†’ tests (red) â†’ tasks (green) â†’ apply (branch + implement + PR)
artifacts:
  - id: proposal
    generates: proposal.md
    description: Initial proposal document outlining the change
    template: proposal.md
    instruction: |
      Create the proposal document that establishes WHY this change is needed.

      Sections:
      - **Why**: 1-2 sentences on the problem or opportunity. What problem does this solve? Why now?
      - **What Changes**: Bullet list of changes. Be specific about new capabilities, modifications, or removals. Mark breaking changes with **BREAKING**.
      - **Capabilities**: Identify which specs will be created or modified:
        - **New Capabilities**: List capabilities being introduced. Each becomes a new `specs/<name>/spec.md`. Use kebab-case names (e.g., `user-auth`, `data-export`).
        - **Modified Capabilities**: List existing capabilities whose REQUIREMENTS are changing. Only include if spec-level behavior changes (not just implementation details). Each needs a delta spec file. Check `openspec/specs/` for existing spec names. Leave empty if no requirement changes.
      - **Impact**: Affected code, APIs, dependencies, or systems.

      IMPORTANT: The Capabilities section is critical. It creates the contract between
      proposal and specs phases. Research existing specs before filling this in.
      Each capability listed here will need a corresponding spec file.

      Keep it concise (1-2 pages). Focus on the "why" not the "how" -
      implementation details belong in design.md.

      This is the foundation - specs, design, and tasks all build on this.
    requires: []

  - id: specs
    generates: "specs/**/*.md"
    description: Detailed specifications for the change
    template: spec.md
    instruction: |
      Create specification files that define WHAT the system should do.

      Create one spec file per capability listed in the proposal's Capabilities section.
      - New capabilities: use the exact kebab-case name from the proposal (specs/<capability>/spec.md).
      - Modified capabilities: use the existing spec folder name from openspec/specs/<capability>/ when creating the delta spec at specs/<capability>/spec.md.

      Delta operations (use ## headers):
      - **ADDED Requirements**: New capabilities
      - **MODIFIED Requirements**: Changed behavior - MUST include full updated content
      - **REMOVED Requirements**: Deprecated features - MUST include **Reason** and **Migration**
      - **RENAMED Requirements**: Name changes only - use FROM:/TO: format

      Format requirements:
      - Each requirement: `### Requirement: <name>` followed by description
      - Use SHALL/MUST for normative requirements (avoid should/may)
      - Each scenario: `#### Scenario: <name>` with WHEN/THEN format
      - **CRITICAL**: Scenarios MUST use exactly 4 hashtags (`####`). Using 3 hashtags or bullets will fail silently.
      - Every requirement MUST have at least one scenario.

      MODIFIED requirements workflow:
      1. Locate the existing requirement in openspec/specs/<capability>/spec.md
      2. Copy the ENTIRE requirement block (from `### Requirement:` through all scenarios)
      3. Paste under `## MODIFIED Requirements` and edit to reflect new behavior
      4. Ensure header text matches exactly (whitespace-insensitive)

      Common pitfall: Using MODIFIED with partial content loses detail at archive time.
      If adding new concerns without changing existing behavior, use ADDED instead.

      Specs should be testable - each scenario maps directly to a test case.
    requires:
      - proposal

  - id: design
    generates: design.md
    description: Technical design document with implementation details
    template: design.md
    instruction: |
      Create the design document that explains HOW to implement the change.

      When to include design.md (create only if any apply):
      - Cross-cutting change (multiple services/modules) or new architectural pattern
      - New external dependency or significant data model changes
      - Security, performance, or migration complexity
      - Ambiguity that benefits from technical decisions before coding

      Sections:
      - **Context**: Background, current state, constraints, stakeholders
      - **Goals / Non-Goals**: What this design achieves and explicitly excludes
      - **Decisions**: Key technical choices with rationale (why X over Y?). Include alternatives considered for each decision.
      - **Risks / Trade-offs**: Known limitations, things that could go wrong. Format: [Risk] â†’ Mitigation
      - **Migration Plan**: Steps to deploy, rollback strategy (if applicable)
      - **Open Questions**: Outstanding decisions or unknowns to resolve

      Focus on architecture and approach, not line-by-line implementation.
      Reference the proposal for motivation and specs for requirements.

      Good design docs explain the "why" behind technical decisions.
    requires:
      - proposal

  - id: tests
    generates: tests.md
    description: "TDD Red phase: write failing tests that define expected behavior"
    template: tests.md
    instruction: |
      Create the test plan following TDD Red phase: define tests that MUST FAIL initially.

      This is the RED phase of TDD (Red â†’ Green â†’ Refactor). Each spec scenario
      maps to one or more test cases. Write tests BEFORE any implementation exists.

      Guidelines:
      - **One test per scenario** from the specs, minimum. Add edge cases as needed.
      - Group tests by capability or feature area using ## numbered headings.
      - Each test MUST be a checkbox: `- [ ] X.Y Test description`
      - Write test descriptions as behavior assertions: "it should...", "it returns...", "it rejects..."
      - Include the test type: `[Feature]`, `[Unit]`, `[Browser]`
      - Reference the spec scenario being tested.

      Test structure for this project (Laravel + Pest):
      - Feature tests: `tests/Feature/` â€” test HTTP endpoints, Livewire components, full request cycle
      - Unit tests: `tests/Unit/` â€” test services, value objects, isolated logic
      - Browser tests: `tests/Browser/` â€” test UI flows with Playwright (Pest browser plugin)

      Example:
      ```
      ## 1. User Authentication

      - [ ] 1.1 [Feature] User can register with valid email and password (Scenario: Successful registration)
      - [ ] 1.2 [Feature] Registration fails with duplicate email (Scenario: Duplicate email rejection)
      - [ ] 1.3 [Unit] Password must meet strength requirements (Scenario: Weak password rejection)
      - [ ] 1.4 [Browser] Registration form shows validation errors inline (Scenario: Form validation UX)
      ```

      After writing this file, the apply phase will:
      1. Create the actual test files with Pest
      2. Run them to confirm they ALL FAIL (red phase)
      3. Only then proceed to implementation (green phase)

      IMPORTANT: Do NOT include implementation tasks here. Only tests.
      Implementation goes in tasks.md (the next artifact).
    requires:
      - specs
      - design

  - id: tasks
    generates: tasks.md
    description: "TDD Green phase: implementation tasks to make failing tests pass"
    template: tasks.md
    instruction: |
      Create the implementation task list â€” the GREEN phase of TDD.

      At this point, failing tests already exist (from the tests artifact).
      Every task here should move one or more tests from RED to GREEN.

      Guidelines:
      - Group related tasks under ## numbered headings.
      - Each task MUST be a checkbox: `- [ ] X.Y Task description`
      - Tasks should be small enough to complete in one session.
      - Order tasks by dependency (what must be done first?).
      - After each task, specify which tests should now pass.
      - Include a final refactor step per group if applicable.

      Structure:
      ```
      ## 1. Database & Models

      - [ ] 1.1 Create migration for X table
      - [ ] 1.2 Create X model with relationships and factory
            â†’ Tests passing: 1.1, 1.2

      ## 2. Service Layer

      - [ ] 2.1 Implement XService with Y method
      - [ ] 2.2 Add error handling for edge cases
            â†’ Tests passing: 2.1, 2.2, 2.3

      ## 3. Refactor & Cleanup

      - [ ] 3.1 Extract shared logic into trait/concern
      - [ ] 3.2 Run full test suite, verify all green
      - [ ] 3.3 Run `composer lint` (Pint + Rector + Prettier)
      ```

      Reference specs for what needs to be built, design for how, and tests
      for what must pass. Each task should be verifiable â€” run the associated
      tests to know when it's done.
    requires:
      - tests
      - design

apply:
  requires: [tasks]
  tracks: tasks.md
  instruction: |
    ## Apply: Branch â†’ Red â†’ Green â†’ PR

    Follow this workflow strictly. Each phase has a GATE that MUST pass before proceeding.

    ### 1. Branch (MANDATORY â€” do this FIRST)
    Create a feature branch for this change BEFORE writing any code:
    ```bash
    git checkout -b change/<change-id>
    ```

    **ðŸš« GATE: No branch, no work.**
    - Verify you are on the correct branch: `git branch --show-current`
    - The branch name MUST match `change/<change-id>`.
    - If you are on `main` or any other branch, STOP and create the branch.
    - Do NOT proceed to Red Phase until the branch exists and is checked out.

    ### 2. Red Phase (from tests.md)
    - Read `tests.md` and create ALL test files using Pest.
    - Run the tests: `php artisan test --compact`
    - **ðŸš« GATE: ALL tests MUST FAIL.**
      - Capture and log the test output showing failures.
      - If ANY test passes, it's not testing new behavior â€” fix the test.
      - Do NOT proceed to Green Phase until every test is RED.
    - Mark each test in `tests.md` as `[x]` once the test FILE is created (not passing â€” just exists and fails).
    - Commit: `git commit -m "test: add failing tests for <change-id>"`

    ### 3. Green Phase (from tasks.md)
    - Work through `tasks.md` sequentially, group by group.
    - **ðŸš« GATE per group: Run associated tests after EACH task group.**
      - Log the test output after each group.
      - Mark tasks as `[x]` in `tasks.md` only when their tests pass.
      - Do NOT move to the next group until current group's tests are GREEN.
    - Also update `tests.md` â€” mark the corresponding test checkboxes `[x]` as they go green.
    - Commit after each group: `git commit -m "feat: <description>"`

    ### 4. Refactor Phase
    - Run full test suite: `composer test`
    - Run linting: `composer lint`
    - **ðŸš« GATE: `composer test` MUST exit 0 (all tests green, PHPStan clean, lint clean, type coverage 100%).**
      - If anything fails, fix it without breaking existing green tests.
      - Re-run `composer test` until fully green.
    - Commit: `git commit -m "refactor: cleanup <change-id>"`

    ### 5. Finalize Artifacts
    - Verify ALL checkboxes in `tests.md` are `[x]`.
    - Verify ALL checkboxes in `tasks.md` are `[x]`.
    - **ðŸš« GATE: Both files must be 100% checked before PR or archive.**

    ### 6. Pull Request
    - Push the branch: `git push origin change/<change-id>`
    - Create PR with summary from proposal.md
    - Link to the change directory for reviewers

    ### Pause Rules
    - Pause if any test fails unexpectedly after going green.
    - Pause if you need to modify a spec (go back to specs artifact).
    - Pause if a dependency or blocker is discovered.
    - Pause if `composer test` fails and the fix is non-trivial.
